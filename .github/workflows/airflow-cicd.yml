name: Airflow CI/CD

on:
  workflow_dispatch:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v3
        
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          pip install apache-airflow
          pip install apache-airflow-providers-apache-kafka
          pip install apache-airflow-providers-postgres
          pip install pandas faker
          pip install -r requirements.txt
          
      - name: Initialize Airflow database
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost/airflow
        run: |
          export AIRFLOW_HOME=$(pwd)
          airflow db init
          airflow users create \
            --username admin \
            --firstname admin \
            --lastname admin \
            --role Admin \
            --email admin@example.com \
            --password admin
            
      - name: Run tests
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost/airflow
        run: |
          pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1
      
      - name: Deploy to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USERNAME: ${{ secrets.EC2_USERNAME }}
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          # Debug Mode
          set -x
          
          # Setup SSH
          echo "Setting up SSH..." >> $GITHUB_STEP_SUMMARY
          mkdir -p /home/runner/.ssh
          echo "$SSH_PRIVATE_KEY" > /home/runner/.ssh/ec2.key
          chmod 600 /home/runner/.ssh/ec2.key
          
          # Set up SSH config to avoid command line issues
          echo "Host ec2
            HostName $EC2_HOST
            User $EC2_USERNAME
            IdentityFile /home/runner/.ssh/ec2.key
            StrictHostKeyChecking no" > /home/runner/.ssh/config
          
          # Test SSH connection
          echo "Testing SSH connection..." >> $GITHUB_STEP_SUMMARY
          ssh ec2 "echo 'SSH connection successful'"
          
          # Check local dags directory
          echo "Checking local dags directory..." >> $GITHUB_STEP_SUMMARY
          if [ ! -d "dags" ] || [ -z "$(ls -A dags)" ]; then
            echo "Error: dags directory is missing or empty" >> $GITHUB_STEP_SUMMARY
            ls -la
            exit 1
          fi
          
          # Prepare remote directory
          echo "Preparing remote directory..." >> $GITHUB_STEP_SUMMARY
          ssh ec2 "mkdir -p /home/$EC2_USERNAME/airflow/dags"
          
          # Deploy DAGs
          echo "Deploying DAGs..." >> $GITHUB_STEP_SUMMARY
          scp -r dags/* ec2:/home/$EC2_USERNAME/airflow/dags/
          
          # Verify deployment
          echo "Verifying deployment..." >> $GITHUB_STEP_SUMMARY
          ssh ec2 "ls -la /home/$EC2_USERNAME/airflow/dags/"
          
          # Restart Airflow with proper initialization
          echo "Restarting Airflow and initializing DAGs..." >> $GITHUB_STEP_SUMMARY
          ssh ec2 "cd /home/$EC2_USERNAME/airflow && \
            docker-compose down && \
            docker-compose up -d && \
            sleep 30 && \
            docker-compose exec -T airflow-webserver airflow dags unpause all && \
            docker-compose exec -T airflow-webserver airflow dags list && \
            docker-compose exec -T airflow-scheduler airflow dags reserialize"
          
          echo "Deployment and DAG initialization completed successfully" >> $GITHUB_STEP_SUMMARY

      - name: Cleanup
        if: always()
        run: |
          rm -f /home/runner/.ssh/ec2.key
          rm -f /home/runner/.ssh/config
