name: Airflow CI/CD
on:
  workflow_dispatch:  # Enable manual triggering
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          pip install apache-airflow
          pip install apache-airflow-providers-apache-kafka
          pip install apache-airflow-providers-postgres
          pip install pandas faker
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
      
      - name: Initialize Airflow database
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost/airflow
        run: |
          export AIRFLOW_HOME=$(pwd)
          airflow db init
          airflow users create \
            --username admin \
            --firstname admin \
            --lastname admin \
            --role Admin \
            --email admin@example.com \
            --password admin
      
      - name: Run tests
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost/airflow
        run: |
          if [ -d "tests" ]; then
            pytest tests/
          else
            echo "No tests directory found, skipping tests"
          fi
  
  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1
      
      - name: Deploy to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USERNAME: ${{ secrets.EC2_USERNAME }}
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          # Setup SSH key
          echo "$SSH_PRIVATE_KEY" > akande_ec2_key.pem
          chmod 600 akande_ec2_key.pem
          
          # Debug: List contents of current directory
          echo "Current directory contents:"
          ls -la
          
          # Check if dags directory exists locally
          if [ ! -d "dags" ]; then
            echo "Error: dags directory not found in repository"
            exit 1
          fi
          
          # Debug: List contents of dags directory
          echo "Contents of dags directory:"
          ls -la dags/
          
          # First ensure the remote directory exists and is empty
          ssh -i akande_ec2_key.pem -o StrictHostKeyChecking=no $EC2_USERNAME@$EC2_HOST << 'EOF'
            # Create airflow directory if it doesn't exist
            mkdir -p ~/airflow/dags
            
            # Backup existing dags if any exist
            if [ "$(ls -A ~/airflow/dags)" ]; then
              timestamp=$(date +%Y%m%d_%H%M%S)
              mkdir -p ~/airflow/dags_backup_$timestamp
              mv ~/airflow/dags/* ~/airflow/dags_backup_$timestamp/ 2>/dev/null || true
            fi
            
            # Debug: Show directory structure
            echo "Remote directory structure:"
            ls -la ~/airflow/
EOF
          
          # Copy DAGs to EC2 using absolute path
          echo "Copying DAGs to EC2..."
          scp -i akande_ec2_key.pem -o StrictHostKeyChecking=no \
            -r dags/* \
            $EC2_USERNAME@$EC2_HOST:/home/$EC2_USERNAME/airflow/dags/
          
          # Verify deployment and restart services
          ssh -i akande_ec2_key.pem -o StrictHostKeyChecking=no $EC2_USERNAME@$EC2_HOST << 'EOF'
            # Check if files were copied
            if [ -z "$(ls -A ~/airflow/dags/)" ]; then
              echo "Error: No files found in destination directory"
              exit 1
            fi
            
            echo "Files in destination directory:"
            ls -la ~/airflow/dags/
            
            # Restart Airflow services
            cd ~/airflow && docker-compose up -d
            
            echo "Airflow services restarted"
EOF
          
          echo "Deployment completed successfully"
      
      - name: Cleanup
        if: always()
        run: rm -f akande_ec2_key.pem
