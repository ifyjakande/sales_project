name: Airflow CI/CD
on:
  workflow_dispatch:
  push:
    branches: 
      - main
  pull_request_review:
    types: [submitted]
    branches: 
      - main       
jobs:
  check-pr-approval:
    if: github.event.review.state == 'approved'
    runs-on: ubuntu-latest
    steps:
      - name: Check PR approval
        run: |
          echo "Pull request was approved, proceeding with workflow"

  check-ec2:
    needs: check-pr-approval
    runs-on: ubuntu-latest
    outputs:
      is-running: ${{ steps.check-status.outputs.status }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1

      - name: Check EC2 status
        id: check-status
        run: |
          STATUS=$(aws ec2 describe-instance-status \
            --instance-ids ${{ secrets.EC2_INSTANCE_ID }} \
            --query 'InstanceStatuses[0].InstanceState.Name' \
            --output text)
          if [ "$STATUS" = "running" ]; then
            echo "status=true" >> "$GITHUB_OUTPUT"
            echo "EC2 instance is running"
          else
            echo "status=false" >> "$GITHUB_OUTPUT"
            echo "EC2 instance is not running"
            exit 1
          fi

  test:
    needs: check-ec2
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: ${{ secrets.DB_USER }}
          POSTGRES_PASSWORD: ${{ secrets.DB_PASSWORD }}
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: --health-cmd pg_isready --health-retries 5
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v3
        
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pytest apache-airflow apache-airflow-providers-apache-kafka apache-airflow-providers-postgres pandas faker
          pip install -r requirements.txt
          
      - name: Initialize Airflow database
        env:
          AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${{ secrets.DB_USER }}:${{ secrets.DB_PASSWORD }}@localhost/airflow
        run: |
          export AIRFLOW_HOME=$(pwd)
          airflow db init
            
      - name: Run tests
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${{ secrets.DB_USER }}:${{ secrets.DB_PASSWORD }}@localhost/airflow
        run: |
          pytest tests/

  deploy:
    needs: [check-ec2, test]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'    # Deploy only when changes are in main
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-north-1
      
      - name: Deploy to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USERNAME: ${{ secrets.EC2_USERNAME }}
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          mkdir -p /home/runner/.ssh
          echo "$SSH_PRIVATE_KEY" > /home/runner/.ssh/ec2.key
          chmod 600 /home/runner/.ssh/ec2.key
          
          echo "Host ec2
            HostName $EC2_HOST
            User $EC2_USERNAME
            IdentityFile /home/runner/.ssh/ec2.key
            StrictHostKeyChecking no" > /home/runner/.ssh/config
          
          # Deploy DAGs and restart services
          ssh ec2 "mkdir -p /home/$EC2_USERNAME/airflow/dags"
          scp -r dags/* ec2:/home/$EC2_USERNAME/airflow/dags/
          ssh ec2 "cd /home/$EC2_USERNAME/airflow && \
            docker-compose down && \
            docker-compose up -d && \
            docker-compose exec -T airflow-webserver airflow dags unpause all"
