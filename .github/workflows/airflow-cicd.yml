name: Airflow CI/CD

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow==2.10.3"
          pip install pytest
          pip install apache-airflow-providers-apache-kafka pandas faker
          pip install -r requirements.txt
      
      - name: Run tests
        run: |
          pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-north-1
      
      - name: Deploy to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          EC2_USERNAME: ${{ secrets.EC2_USERNAME }}
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          echo "$SSH_PRIVATE_KEY" > akande_ec2_key.pem
          chmod 600 akande_ec2_key.pem
          
          # Copy DAGs to EC2
          scp -i akande_ec2_key.pem -o StrictHostKeyChecking=no \
            -r dags/* \
            $EC2_USERNAME@$EC2_HOST:AIRFLOW/dags/
          
          # Restart Airflow services
          ssh -i akande_ec2_key.pem -o StrictHostKeyChecking=no $EC2_USERNAME@$EC2_HOST \
            "cd AIRFLOW && docker-compose restart airflow-webserver airflow-scheduler"
